{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Base):\n",
    "    \n",
    "\n",
    "    def load_dataset(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "    \n",
    "    def text_preprocess(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        #remove hyperlinks\n",
    "        tweet = re.sub(r\"http?:\\S+|www\\S+|https?:\\S+\", '', tweet)\n",
    "        #remove user @ mentions\n",
    "        tweet = re.sub(r'@[a-z0-9]+','', tweet)\n",
    "        #remove # symbols\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        #remove rt\n",
    "        tweet = re.sub(r'rt[\\s]+', '', tweet)\n",
    "        #remove punctuations\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        #remove stopwords\n",
    "        tweet_tokens = word_tokenize(tweet)\n",
    "        filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "        #word normalization\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        #stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "    \n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def get_feature_vector(train_fit):\n",
    "        vector = TfidfVectorizer(sublinear_tf=True)\n",
    "        vector.fit(train_fit)\n",
    "        return vector\n",
    "    def model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
